<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png"><link rel="icon" href="/img/favicon.ico"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content=""><meta name="author" content="Guoliang"><meta name="keywords" content=""><title>Spark SQL 执行流程 - Guoliang&#39;s Blog</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.6.0/styles/a11y-dark.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"example.com",root:"/",version:"1.8.10",typing:{enable:!0,typeSpeed:80,cursorChar:"乄",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"always",icon:"❡"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"gffo4y4h3e1D9tHT2ADcTxtp-gzGzoHsz",app_key:"EaplHXnXAqIYs5wCxGMVB4l4",server_url:"https://gffo4y4h.lc-cn-n1-shared.com"}}}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.2"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>步尽白</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(/img/pingan.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="Spark SQL 执行流程"></span><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> Guoliang </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2021-04-06 17:24" pubdate>2021年4月6日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 1.9k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 12 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">Spark SQL 执行流程</h1><p class="note note-info">本文最后更新于：2025年1月20日 下午</p><div class="markdown-body"><h1 id="Spark-SQL-执行流程"><a href="#Spark-SQL-执行流程" class="headerlink" title="Spark SQL 执行流程"></a>Spark SQL 执行流程</h1><p>一般来说，从 SQL 转换到 RDD 执行需要经过两个大阶段，分别是逻辑计划（LogicalPlan）和物理计划（SparkPlan），而在整个 Spark 的执行过程中，其代码都是惰性的，即到最后 SQL 真正执行的时候，整个代码才会从后向前按调用的依赖顺序执行。</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ol><li><p>逻辑计划</p><ul><li><p><code>Unresolved LogicalPlan</code>：仅仅是数据结构，不包含具体数据</p></li><li><p><code>Analyzed LogicalPlan</code>：绑定与数据对应的具体信息</p></li><li><p><code>Optimized LogicalPlan</code>：应用优化规则</p></li></ul></li><li><p>物理计划</p><ul><li><p><code>Iterator[PhysicalPlan]</code>：生成物理算子树的列表</p></li><li><p><code>SparkPlan</code>：按照策略选取最优的物理算子树</p></li><li><p><code>Prepared SparkPlan</code>：进行提交前的准备工作</p></li></ul></li></ol><h2 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h2><p><img src="https://cdn.jsdelivr.net/gh/gleonSun/images@main/image/SQLExecutionFlow-1737348368885.png" srcset="/img/loading.gif" lazyload></p><h1 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h1><p>SparkSession 类中的 sql 方法是 Spark 执行 SQL 查询的入口，sqlText 即为用户输入的 SQL 语句，其中 parsePlan 方法则是对 SQL 语句进行解析，Spark 使用的编译器语法基于 ANTLR4 这一工具，下文会稍微提及此部分内容。</p><div class="hljs code-wrapper"><pre><code class="hljs java">def <span class="hljs-title function_">sql</span><span class="hljs-params">(sqlText: String)</span>: DataFrame = &#123;
    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))
&#125;</code></pre></div><h2 id="Parser"><a href="#Parser" class="headerlink" title="Parser"></a>Parser</h2><h3 id="ANTLR4"><a href="#ANTLR4" class="headerlink" title="ANTLR4"></a>ANTLR4</h3><p>ANTLR4 有两种遍历模式，一种是监听模式，属于被动型的；另一种是访问者模式，属于主动型的，这也是 Spark 使用的遍历模式，可以显示地定义遍历语法树的顺序。</p><p>在 Spark 中体现为 <span class="label label-primary">SqlBase.g4</span> 文件，包含词法分析器（SqlBaseLexer）、语法分析器（SqlBaseParser）和访问者类（SqlBaseVisitor 接口与 SqlBaseBaseVisitor 类）。</p><p>也就是说，如果用户需要增加新的语法，在 <span class="label label-primary">SqlBase.g4</span> 文件中增加相应语法和词法后，重新编译后即增加了新的语法句式，然后便可以基于 AstBuilder（SparkSqlAstBuilder） 中对新增的语法进行逻辑补充，这种可以直接执行的都属于 Command，后面会以实例进行说明。</p><h3 id="AbstractSqlParser"><a href="#AbstractSqlParser" class="headerlink" title="AbstractSqlParser"></a>AbstractSqlParser</h3><p>Spark SQL 中的 Catalyst 中提供了直接面向用户的 ParserInterface 接口，该接口中包含了对 SQL 语句、Expression 表达式和 TableIdentifier 数据表标识符等的解析方法。AbstractSqlParser 继承了 ParserInterface，主要借助 AstBuilder 对语法树进行解析（遵循后序遍历方式）。</p><h3 id="SQL-实例"><a href="#SQL-实例" class="headerlink" title="SQL 实例"></a>SQL 实例</h3><p>以 IDEA 为例，先安装 ANTLR4 的插件，然后右键选择 <span class="label label-primary">singleStatement</span>，点击 <span class="label label-secondary">Test Rule singleStatement</span> 进行调试，这里我们输入一个简单的 SQL：<span class="label label-success">DROP TABLE IF EXISTS SPARKTEST</span></p><div class="note note-warning"><p>此处的所有字母均为大写，因为这里对应的语法和词法是区分大小写的，我们仅仅是在调试对应的 SQL 语法树，Spark 在后面的解析中利用 UpperCaseCharStream 才会将 SQL 都转为大写进行处理，所以用户的 SQL 语句不需要大写，如下图。</p></div><p><img src="https://cdn.jsdelivr.net/gh/gleonSun/images@main/image/drop_sql-1737348368879.png" srcset="/img/loading.gif" lazyload></p><p>现在我们来自定义一条 SQL 语法：<span class="label label-success">SHOW STATUS</span></p><ul><li>修改 <span class="label label-primary">SqlBase.g4</span> 文件，新增词法和语法</li></ul><p><img src="https://cdn.jsdelivr.net/gh/gleonSun/images@main/image/sql_yf-1737348368884.png" srcset="/img/loading.gif" lazyload><br><img src="https://cdn.jsdelivr.net/gh/gleonSun/images@main/image/sql_cf-1737348368882.png" srcset="/img/loading.gif" lazyload></p><ul><li>这里我是整个项目编译的，因为之前编译的不小心清除了，只需要编译 Spark SQL 模块即可</li></ul><p><img src="https://cdn.jsdelivr.net/gh/gleonSun/images@main/image/sql_by-1737348368880.png" srcset="/img/loading.gif" lazyload></p><ul><li>编写相应逻辑的 ShowStatusCommand 样例类</li></ul><p><img src="https://cdn.jsdelivr.net/gh/gleonSun/images@main/image/sql_yl-1737348368884.png" srcset="/img/loading.gif" lazyload></p><ul><li>在 SparkSqlAstBuilder 中增加对外接口</li></ul><p><img src="https://cdn.jsdelivr.net/gh/gleonSun/images@main/image/sql_jk-1737348368882.png" srcset="/img/loading.gif" lazyload></p><ul><li>通过 Spark API 使用 SQL 输出结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/gleonSun/images@main/image/sql_api-1737348368880.png" srcset="/img/loading.gif" lazyload><br><img src="https://cdn.jsdelivr.net/gh/gleonSun/images@main/image/sql_result-1737348368883.png" srcset="/img/loading.gif" lazyload></p><h2 id="Logical-Spark-Plan"><a href="#Logical-Spark-Plan" class="headerlink" title="Logical/Spark Plan"></a>Logical/Spark Plan</h2><p>在 Dataset 中继续对未解析的逻辑计划进行解析，本文仅针对 Command 部分的逻辑计划举例分析。</p><div class="hljs code-wrapper"><pre><code class="hljs java">def <span class="hljs-title function_">ofRows</span><span class="hljs-params">(sparkSession: SparkSession, logicalPlan: LogicalPlan)</span>: DataFrame = &#123;
    <span class="hljs-type">val</span> <span class="hljs-variable">qe</span> <span class="hljs-operator">=</span> sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    <span class="hljs-keyword">new</span> <span class="hljs-title class_">Dataset</span>[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
&#125;</code></pre></div><p>可以看到，在 QueryExecution 中，将未解析的逻辑计划转换为解析的逻辑计划，详细代码在 Analyzer 的 executeAndCheck 方法中，最终调用了特质 CheckAnalysis 的 checkAnalysis 方法进行数据的绑定解析，代码较长此处就不贴了，这一过程会对应表（Relation）、Where 后的过滤条件（Filter）、查询的列（Project）、别名（Cast）等等进行绑定，解析失败会抛出相应错误。</p><div class="hljs code-wrapper"><pre><code class="hljs java">def <span class="hljs-title function_">assertAnalyzed</span><span class="hljs-params">()</span>: Unit = analyzed

lazy val analyzed: LogicalPlan = &#123;
    SparkSession.setActiveSession(sparkSession)
    sparkSession.sessionState.analyzer.executeAndCheck(logical)
&#125;</code></pre></div><p>当执行到 Dataset 中 ofRows 最后一行 <code>new Dataset[Row](...)</code> 时，会调用到初始化 logicalPlan 的地方，到这里开始向前追溯，需要判断当前解析后的逻辑计划是 Command 还是其他的逻辑计划。</p><div class="note note-warning"><p>Command 在 Spark 中比较特殊，可以直接在 Driver 端执行，此处我们基于上面的 DropTableCommand 来分析。</p></div><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@transient</span> <span class="hljs-keyword">private</span>[sql] val logicalPlan: LogicalPlan = &#123;
    queryExecution.analyzed match &#123;
      <span class="hljs-keyword">case</span> c: Command =&gt;
        LocalRelation(c.output, withAction(<span class="hljs-string">&quot;command&quot;</span>, queryExecution)(_.executeCollect()))
      <span class="hljs-keyword">case</span> u @ Union(children) <span class="hljs-keyword">if</span> children.forall(_.isInstanceOf[Command]) =&gt;
        LocalRelation(u.output, withAction(<span class="hljs-string">&quot;command&quot;</span>, queryExecution)(_.executeCollect()))
      <span class="hljs-type">case</span> <span class="hljs-variable">_</span> <span class="hljs-operator">=</span>&gt;
        queryExecution.analyzed
    &#125;
  &#125;</code></pre></div><p>往下继续执行，无论是 Command 还是其他的逻辑计划，均会经历下面的过程，不同的是 Command 直接就执行了，而其他的逻辑计划如查询等则会经历更多的变换过程直至发往 Executor 执行。</p><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">private</span> def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan =&gt; U) = &#123;
    <span class="hljs-keyword">try</span> &#123;
      qe.executedPlan.foreach &#123; plan =&gt;
        plan.resetMetrics()
      &#125;
      <span class="hljs-type">val</span> <span class="hljs-variable">start</span> <span class="hljs-operator">=</span> System.nanoTime()
      <span class="hljs-type">val</span> <span class="hljs-variable">result</span> <span class="hljs-operator">=</span> SQLExecution.withNewExecutionId(sparkSession, qe) &#123;
        action(qe.executedPlan)
      &#125;
      <span class="hljs-type">val</span> <span class="hljs-variable">end</span> <span class="hljs-operator">=</span> System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    &#125; <span class="hljs-keyword">catch</span> &#123;
      <span class="hljs-keyword">case</span> e: Exception =&gt;
        sparkSession.listenerManager.onFailure(name, qe, e)
        <span class="hljs-keyword">throw</span> e
    &#125;
&#125;</code></pre></div><p>前面说过，Spark 是惰性执行的，我们看一下 QueryExecution 中的部分代码，当真正需要执行的时候才会从 Prepared SparkPlan 向前追溯并按照依赖顺序执行，中间还有很多过程，包括优化逻辑计划，运用策略转换物理计划，选取最优物理计划等等，以下是逻辑计划和物理计划的转换过程部分代码。</p><div class="hljs code-wrapper"><pre><code class="hljs java">lazy val executedPlan: SparkPlan = prepareForExecution(sparkPlan)

lazy val sparkPlan: SparkPlan = &#123;
    SparkSession.setActiveSession(sparkSession)
    planner.plan(ReturnAnswer(optimizedPlan)).next()
&#125;

lazy val optimizedPlan: LogicalPlan = sparkSession.sessionState.optimizer.execute(withCachedData)

lazy val withCachedData: LogicalPlan = &#123;
    assertAnalyzed()
    assertSupported()
    sparkSession.sharedState.cacheManager.useCachedData(analyzed)
&#125;

lazy val analyzed: LogicalPlan = &#123;
    SparkSession.setActiveSession(sparkSession)
    sparkSession.sessionState.analyzer.executeAndCheck(logical)
&#125;</code></pre></div><h3 id="Command"><a href="#Command" class="headerlink" title="Command"></a>Command</h3><p>接上面 Dataset 里 logicalPlan 中的 <code>_.executeCollect()</code> 方法，由于是可执行 Command，所以调用至 ExecutedCommandExec 的 executeCollect 方法继续执行，最终调用了 DropTableCommand 的 run 方法执行。</p><div class="hljs code-wrapper"><pre><code class="hljs java">override def <span class="hljs-title function_">executeCollect</span><span class="hljs-params">()</span>: Array[InternalRow] = sideEffectResult.toArray

<span class="hljs-keyword">protected</span>[sql] lazy val sideEffectResult: Seq[InternalRow] = &#123;
    <span class="hljs-type">val</span> <span class="hljs-variable">converter</span> <span class="hljs-operator">=</span> CatalystTypeConverters.createToCatalystConverter(schema)
    cmd.run(sqlContext.sparkSession).map(converter(_).asInstanceOf[InternalRow])
&#125;</code></pre></div><p>DropTableCommand 继承了 RunnableCommand，而 RunnableCommand 则包装在 ExecutedCommandExec 中，下面的代码可以看到，先根据表名取出对应表的元数据信息，然后清除缓存并刷新缓存状态，再调用 SessionCatalog 的 dropTable 方法，如果是 Hive 表，则会调用 externalCatalog（HiveExternalCatalog）的 dropTable 方法对表进行删除。</p><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">case</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">DropTableCommand</span>(
    tableName: TableIdentifier,
    ifExists: Boolean,
    isView: Boolean,
    purge: Boolean) <span class="hljs-keyword">extends</span> <span class="hljs-title class_">RunnableCommand</span> &#123;

  override def <span class="hljs-title function_">run</span><span class="hljs-params">(sparkSession: SparkSession)</span>: Seq[Row] = &#123;
    <span class="hljs-type">val</span> <span class="hljs-variable">catalog</span> <span class="hljs-operator">=</span> sparkSession.sessionState.catalog
    <span class="hljs-type">val</span> <span class="hljs-variable">isTempView</span> <span class="hljs-operator">=</span> catalog.isTemporaryTable(tableName)

    <span class="hljs-keyword">if</span> (!isTempView &amp;&amp; catalog.tableExists(tableName)) &#123;
      catalog.getTableMetadata(tableName).tableType match &#123;
        <span class="hljs-keyword">case</span> CatalogTableType.VIEW <span class="hljs-keyword">if</span> !isView =&gt;
          <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">AnalysisException</span>(
            <span class="hljs-string">&quot;Cannot drop a view with DROP TABLE. Please use DROP VIEW instead&quot;</span>)
        <span class="hljs-keyword">case</span> o <span class="hljs-keyword">if</span> o != CatalogTableType.VIEW &amp;&amp; isView =&gt;
          <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">AnalysisException</span>(
            s<span class="hljs-string">&quot;Cannot drop a table with DROP VIEW. Please use DROP TABLE instead&quot;</span>)
        <span class="hljs-type">case</span> <span class="hljs-variable">_</span> <span class="hljs-operator">=</span>&gt;
      &#125;
    &#125;

    <span class="hljs-keyword">if</span> (isTempView || catalog.tableExists(tableName)) &#123;
      <span class="hljs-keyword">try</span> &#123;
        sparkSession.sharedState.cacheManager.uncacheQuery(
          sparkSession.table(tableName), cascade = !isTempView)
      &#125; <span class="hljs-keyword">catch</span> &#123;
        <span class="hljs-keyword">case</span> <span class="hljs-title function_">NonFatal</span><span class="hljs-params">(e)</span> =&gt; log.warn(e.toString, e)
      &#125;
      catalog.refreshTable(tableName)
      catalog.dropTable(tableName, ifExists, purge)
    &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (ifExists) &#123;
      <span class="hljs-comment">// no-op</span>
    &#125; <span class="hljs-keyword">else</span> &#123;
      <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">AnalysisException</span>(s<span class="hljs-string">&quot;Table or view not found: $&#123;tableName.identifier&#125;&quot;</span>)
    &#125;
    Seq.empty[Row]
  &#125;
&#125;</code></pre></div><p>Spark SQL 中的 Catalog 体系实现以 SessionCatalog 为主体，通过 SparkSession 提供给外部调用，它起到了一个代理的作用，对底层的元数据信息、临时表信息、视图信息和函数信息进行了封装。初始化过程在 BaseSessionStateBuilder 类，而 externalCatalog 则是基于配置参数 <code>spark.sql.catalogImplementation</code> 进行匹配选择的，代码位于 SharedState 类，默认是 <code>in-memory</code> 即内存模式，可选的是 <code>hive</code> 模式，至此 DropTableCommand 分析完毕。</p><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">protected</span> lazy val catalog: SessionCatalog = &#123;
    <span class="hljs-type">val</span> <span class="hljs-variable">catalog</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">SessionCatalog</span>(
      () =&gt; session.sharedState.externalCatalog,
      () =&gt; session.sharedState.globalTempViewManager,
      functionRegistry,
      conf,
      SessionState.newHadoopConf(session.sparkContext.hadoopConfiguration, conf),
      sqlParser,
      resourceLoader)
    parentState.foreach(_.catalog.copyStateTo(catalog))
    catalog
  &#125;</code></pre></div></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/">分布式系统</a> <a class="hover-with-bg" href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/">分布式计算</a> <a class="hover-with-bg" href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/Spark/">Spark</a></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a class="hover-with-bg" href="/tags/Spark/">Spark</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2021/04/13/Spark-%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Spark 调度系统</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/04/02/Spark-RDD/"><span class="hidden-mobile">Spark RDD</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="valine"></div><script type="text/javascript">Fluid.utils.loadComments("#valine",(function(){Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js",(function(){new Valine({el:"#valine",app_id:"gffo4y4h3e1D9tHT2ADcTxtp-gzGzoHsz",app_key:"EaplHXnXAqIYs5wCxGMVB4l4",placeholder:"说点什么",path:window.location.pathname,avatar:"robohash",meta:["nick","mail"],pageSize:"10",lang:"zh-CN",highlight:!0,recordIP:!0,serverURLs:""})}))}))</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div><div class="statistics"><span id="leancloud-site-pv-container" style="display:none">总访问量 <span id="leancloud-site-pv"></span> 次 </span><span id="leancloud-site-uv-container" style="display:none">总访客数 <span id="leancloud-site-uv"></span> 人</span></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.2/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script defer src="/js/leancloud.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script><script>!function(t,i){(0,Fluid.plugins.typing)(i.getElementById("subtitle").title)}(window,document)</script><script src="/js/local-search.js"></script><script>$("#local-search-input").on("click",(function(){searchFunc("/local-search.xml","local-search-input","local-search-result")})),$("#modalSearch").on("shown.bs.modal",(function(){$("#local-search-input").focus()}))</script><script src="/js/boot.js"></script></body></html>